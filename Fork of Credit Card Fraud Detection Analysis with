{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* Explanation of Each Part\n\n- Data Loading and Exploration:\n\n    + Load the dataset.\n    + Display basic information and descriptive statistics.\n    + Check for and handle missing values.\n\n- Data Preprocessing:\n\n    + Split the dataset into features (X) and target (y).\n    + Split the data into training and testing sets.\n\n- Handling Imbalanced Data with SMOTE:\n\n    + Apply SMOTE to address class imbalance.\n\n- Feature Scaling:\n\n    + Standardize the features using StandardScaler.\n\n- Logistic Regression Model Training and Evaluation:\n\n    + Train a Logistic Regression model.\n    + Evaluate the model using classification metrics.\n\n- Hyperparameter Tuning with GridSearchCV:\n\n    + Perform hyperparameter tuning using GridSearchCV.\n    + Evaluate the best model obtained from the grid search.\n\n- Feature Importance Analysis with RandomForestClassifier:\n\n    + Train a Random Forest model.\n    + Extract and display feature importance.\n\n- Visualization:\n\n    + Visualize the top 10 most important features.\n    + Plot correlation matrix.\n    + Plot per column distribution for selected columns.","metadata":{}},{"cell_type":"code","source":"# Part 1: Data Loading and Exploration\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\ndf = pd.read_csv(r\"/kaggle/input/creditcardfraud/creditcard.csv\")\n\n# Basic Data Exploration\nprint(\"Dataset Information:\")\nprint(df.info())\nprint(\"\\nDataset Description:\")\nprint(df.describe())\n\n# Check for missing values\nprint(\"\\nMissing Values in Each Column:\")\nmissing_values = df.isnull().sum()\nprint(missing_values)\n\n# Handle missing values (if any)\nif missing_values.any():\n    print(\"Dropping rows with missing values\")\n    df = df.dropna()\n\n# Part 2: Data Preprocessing\n\n# Split features and target\nX = df.drop(['Class'], axis=1)\ny = df['Class']\n\n# Splitting the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Part 3: Handling Imbalanced Data with SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\n# Apply SMOTE to balance the dataset\nprint(\"Applying SMOTE to balance the dataset\")\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# Part 4: Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize the features\nprint(\"Standardizing the features\")\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_smote)\nX_test_scaled = scaler.transform(X_test)\n\n# Part 5: Logistic Regression Model Training and Evaluation\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Function to evaluate the model\ndef evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n    \"\"\"Evaluate the model and print performance metrics.\"\"\"\n    y_pred = model.predict(X_test)\n    print(f\"\\n{model_name} Evaluation:\")\n    print(classification_report(y_test, y_pred))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n    print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred))\n    return y_pred\n\n# Train Logistic Regression model\nprint(\"Training Logistic Regression model\")\nlogreg = LogisticRegression(max_iter=1000, random_state=42)\nlogreg.fit(X_train_scaled, y_train_smote)\n\n# Evaluate Logistic Regression\nevaluate_model(logreg, X_test_scaled, y_test, \"Logistic Regression\")\n\n# Part 6: Hyperparameter Tuning with GridSearchCV\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define hyperparameters for tuning\nprint(\"Hyperparameter tuning with GridSearchCV\")\nparam_grid = {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\ngrid_search = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid, cv=5, scoring='roc_auc')\ngrid_search.fit(X_train_scaled, y_train_smote)\n\n# Use the best model for prediction and evaluation\nbest_model = grid_search.best_estimator_\nevaluate_model(best_model, X_test_scaled, y_test, \"Best Logistic Regression\")\n\n# Part 7: Feature Importance Analysis with RandomForestClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train Random Forest model\nprint(\"Training Random Forest model for feature importance analysis\")\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train_smote)\n\n# Extract feature importance\nimportance = rf_model.feature_importances_\nfeature_names = X.columns.tolist()\nfeature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nFeature Importance Analysis:\")\nprint(feature_importance_df)\n\n# Part 8: Visualization\n\n# Visualization of feature importance\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df[:10])\nplt.title('Top 10 Most Important Features')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.show()\n\n# Function to plot correlation matrix\ndef plot_correlation_matrix(df, graph_width):\n    \"\"\"Plot correlation matrix.\"\"\"\n    plt.figure(figsize=(graph_width, graph_width))\n    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix')\n    plt.show()\n\nplot_correlation_matrix(df, 10)\n\n# Function to plot per column distribution\ndef plot_per_column_distribution(df, n_graph_shown, n_graph_per_row):\n    \"\"\"Plot per column distribution for columns with 1 to 50 unique values.\"\"\"\n    nunique = df.nunique()\n    df = df[[col for col in df if 1 < nunique[col] < 50]]  # Filter columns with unique values\n    n_row, n_col = df.shape\n    column_names = list(df)\n    n_graph_row = int(np.ceil(n_col / n_graph_per_row))\n    plt.figure(figsize=(6 * n_graph_per_row, 8 * n_graph_row), dpi=80, facecolor='w', edgecolor='k')\n    for i in range(min(n_col, n_graph_shown)):\n        plt.subplot(n_graph_row, n_graph_per_row, i + 1)\n        column_df = df.iloc[:, i]\n        if not column_df.empty and not column_df.isnull().all():\n            if not column_df.dtype.kind in 'bifc':\n                column_df = column_df.astype(float)\n            column_df.hist()\n        plt.ylabel('Counts')\n        plt.xticks(rotation=90)\n        plt.title(f'{column_names[i]} (column {i})')\n    plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)\n    plt.show()\n\nplot_per_column_distribution(df, 10, 5)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T19:37:56.928859Z","iopub.execute_input":"2024-05-17T19:37:56.929407Z","iopub.status.idle":"2024-05-17T19:51:07.473924Z","shell.execute_reply.started":"2024-05-17T19:37:56.929366Z","shell.execute_reply":"2024-05-17T19:51:07.472579Z"},"trusted":true},"execution_count":null,"outputs":[]}]}